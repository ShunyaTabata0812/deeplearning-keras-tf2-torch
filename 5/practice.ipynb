{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sin(x,T = 100):\n",
    "    return np.sin(2.0*np.pi*x/T)\n",
    "def toy_problem(T=100,ampl = 0.05):\n",
    "    x = np.arange(0,2*T+1)\n",
    "    noise =  ampl * np.random.uniform(low =-1.0, high = 1.0, size = len(x))\n",
    "    return sin(x)+noise\n",
    "\n",
    "T = 100\n",
    "f = toy_problem(T).astype(np.float32)\n",
    "length_of_sequences = len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 25\n",
    "x = []\n",
    "t = []\n",
    "for i in range(length_of_sequences-maxlen):\n",
    "    x.append(f[i:i+maxlen])\n",
    "    t.append(f[i+maxlen])\n",
    "    \n",
    "x = np.array(x).reshape(-1,maxlen,1)\n",
    "t = np.array(t).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,t_train,t_val = train_test_split(x,t,test_size = 0.2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(50,activation='tanh',\n",
    "                    kernel_initializer='glorot_normal', #入力層、隠れ層間の重みの初期値　tanhの場合はXavierの初期化\n",
    "                    recurrent_initializer='orthogonal' # 過去の隠れ層からの重みの初期値　直行座標\n",
    "                   ))\n",
    "model.add(Dense(1,activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 140 samples, validate on 36 samples\n",
      "Epoch 1/1000\n",
      "140/140 - 4s - loss: 0.9622 - val_loss: 0.4381\n",
      "Epoch 2/1000\n",
      "140/140 - 0s - loss: 0.5030 - val_loss: 0.2008\n",
      "Epoch 3/1000\n",
      "140/140 - 0s - loss: 0.2356 - val_loss: 0.1002\n",
      "Epoch 4/1000\n",
      "140/140 - 0s - loss: 0.0976 - val_loss: 0.0537\n",
      "Epoch 5/1000\n",
      "140/140 - 0s - loss: 0.0431 - val_loss: 0.0309\n",
      "Epoch 6/1000\n",
      "140/140 - 0s - loss: 0.0323 - val_loss: 0.0193\n",
      "Epoch 7/1000\n",
      "140/140 - 0s - loss: 0.0347 - val_loss: 0.0133\n",
      "Epoch 8/1000\n",
      "140/140 - 0s - loss: 0.0366 - val_loss: 0.0103\n",
      "Epoch 9/1000\n",
      "140/140 - 0s - loss: 0.0310 - val_loss: 0.0103\n",
      "Epoch 10/1000\n",
      "140/140 - 0s - loss: 0.0206 - val_loss: 0.0122\n",
      "Epoch 11/1000\n",
      "140/140 - 0s - loss: 0.0130 - val_loss: 0.0145\n",
      "Epoch 12/1000\n",
      "140/140 - 0s - loss: 0.0100 - val_loss: 0.0152\n",
      "Epoch 13/1000\n",
      "140/140 - 0s - loss: 0.0115 - val_loss: 0.0146\n",
      "Epoch 14/1000\n",
      "140/140 - 0s - loss: 0.0133 - val_loss: 0.0127\n",
      "Epoch 15/1000\n",
      "140/140 - 0s - loss: 0.0128 - val_loss: 0.0103\n",
      "Epoch 16/1000\n",
      "140/140 - 0s - loss: 0.0101 - val_loss: 0.0079\n",
      "Epoch 17/1000\n",
      "140/140 - 0s - loss: 0.0072 - val_loss: 0.0062\n",
      "Epoch 18/1000\n",
      "140/140 - 0s - loss: 0.0052 - val_loss: 0.0055\n",
      "Epoch 19/1000\n",
      "140/140 - 0s - loss: 0.0054 - val_loss: 0.0053\n",
      "Epoch 20/1000\n",
      "140/140 - 0s - loss: 0.0061 - val_loss: 0.0046\n",
      "Epoch 21/1000\n",
      "140/140 - 0s - loss: 0.0059 - val_loss: 0.0039\n",
      "Epoch 22/1000\n",
      "140/140 - 0s - loss: 0.0051 - val_loss: 0.0040\n",
      "Epoch 23/1000\n",
      "140/140 - 0s - loss: 0.0042 - val_loss: 0.0043\n",
      "Epoch 24/1000\n",
      "140/140 - 0s - loss: 0.0037 - val_loss: 0.0044\n",
      "Epoch 25/1000\n",
      "140/140 - 0s - loss: 0.0036 - val_loss: 0.0043\n",
      "Epoch 26/1000\n",
      "140/140 - 0s - loss: 0.0037 - val_loss: 0.0041\n",
      "Epoch 27/1000\n",
      "140/140 - 0s - loss: 0.0036 - val_loss: 0.0037\n",
      "Epoch 28/1000\n",
      "140/140 - 0s - loss: 0.0032 - val_loss: 0.0033\n",
      "Epoch 29/1000\n",
      "140/140 - 0s - loss: 0.0029 - val_loss: 0.0031\n",
      "Epoch 30/1000\n",
      "140/140 - 0s - loss: 0.0029 - val_loss: 0.0030\n",
      "Epoch 31/1000\n",
      "140/140 - 0s - loss: 0.0030 - val_loss: 0.0029\n",
      "Epoch 32/1000\n",
      "140/140 - 0s - loss: 0.0029 - val_loss: 0.0027\n",
      "Epoch 33/1000\n",
      "140/140 - 0s - loss: 0.0027 - val_loss: 0.0027\n",
      "Epoch 34/1000\n",
      "140/140 - 0s - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 35/1000\n",
      "140/140 - 0s - loss: 0.0026 - val_loss: 0.0026\n",
      "Epoch 36/1000\n",
      "140/140 - 0s - loss: 0.0026 - val_loss: 0.0025\n",
      "Epoch 37/1000\n",
      "140/140 - 0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 38/1000\n",
      "140/140 - 0s - loss: 0.0025 - val_loss: 0.0025\n",
      "Epoch 39/1000\n",
      "140/140 - 0s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 40/1000\n",
      "140/140 - 0s - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 41/1000\n",
      "140/140 - 0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 42/1000\n",
      "140/140 - 0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 43/1000\n",
      "140/140 - 0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 44/1000\n",
      "140/140 - 0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 45/1000\n",
      "140/140 - 0s - loss: 0.0024 - val_loss: 0.0023\n",
      "Epoch 46/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 47/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 48/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 49/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 50/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 51/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 52/1000\n",
      "140/140 - 0s - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 53/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 54/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 55/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 56/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 57/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 58/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0022\n",
      "Epoch 59/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 60/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 61/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 62/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 63/1000\n",
      "140/140 - 0s - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 64/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 65/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 66/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 67/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 68/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 69/1000\n",
      "140/140 - 0s - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 70/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 71/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 72/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 73/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 74/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 75/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 76/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 77/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 78/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 79/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 80/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 81/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 82/1000\n",
      "140/140 - 0s - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 83/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 84/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 85/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 86/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 87/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 88/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 89/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 90/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 91/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 92/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 93/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 94/1000\n",
      "140/140 - 0s - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 95/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 96/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 97/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 98/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 99/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 100/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 101/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 102/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 103/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 104/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 105/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 106/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 107/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 108/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 109/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 110/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 111/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 112/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 113/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 114/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 115/1000\n",
      "140/140 - 0s - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 116/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 117/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 118/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 119/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 120/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 121/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 122/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 123/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 124/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 125/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 126/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 127/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 128/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 129/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 130/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 131/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 132/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 133/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 134/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 135/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 136/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 137/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 138/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 139/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 140/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 141/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 142/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 143/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 144/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 145/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 146/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 147/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 148/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 149/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 150/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 151/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 152/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 153/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 154/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 155/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 156/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 157/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 158/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 159/1000\n",
      "140/140 - 0s - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 160/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 161/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 162/1000\n",
      "140/140 - 0s - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 163/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 164/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 165/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 166/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 167/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 168/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 169/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 170/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 171/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 172/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 173/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 174/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 175/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 176/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 177/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 178/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 179/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 180/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 181/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 182/1000\n",
      "140/140 - 0s - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 183/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 184/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 185/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 186/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 187/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 188/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 189/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 190/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 191/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 192/1000\n",
      "140/140 - 0s - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 00192: early stopping\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,amsgrad=True)\n",
    "model.compile(optimizer = optimizer,loss = 'mean_squared_error')\n",
    "es = EarlyStopping(monitor='val_loss',patience=10, verbose=1)\n",
    "hist = model.fit(x_train, t_train,epochs = 1000,batch_size = 100,verbose =2, validation_data = (x_val,t_val),callbacks = [es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sin = toy_problem(T, ampl = 0)\n",
    "gen = [None for i in range(maxlen)]\n",
    "z = x[:1]\n",
    "\n",
    "for i in range(length_of_sequences - maxlen):\n",
    "    preds = model.predict(z[-1:])\n",
    "    z = np.append(z, preds)[1:]\n",
    "    z = z.reshape(-1,maxlen,1)\n",
    "    gen.append(preds[0,0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffbbf1700b8>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.rc('font',family = 'serif')\n",
    "plt.xlim([0,2*T])\n",
    "plt.ylim([-1.5,1.5])\n",
    "plt.plot(range(len(f)),sin,color = 'gray',linestyle = '--',linewidth = 0.5)\n",
    "\n",
    "plt.plot(range(len(f)),gen, color = 'black',linewidth = 1,marker = 'o', markersize = 1, markerfacecolor = 'black', markeredgecolor = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = SimpleRNN(hidden_dim, activation='tanh',\n",
    "                            kernel_initializer='glorot_normal',\n",
    "                            recurrent_initializer='orthogonal')\n",
    "        self.l2 = Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        h = self.l1(x)\n",
    "        y = self.l2(h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデルの構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル学種"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.378, val_loss: 0.212\n",
      "epoch: 2, loss: 0.276, val_loss: 0.149\n",
      "epoch: 3, loss: 0.207, val_loss: 0.109\n",
      "epoch: 4, loss: 0.162, val_loss: 0.091\n",
      "epoch: 5, loss: 0.136, val_loss: 0.083\n",
      "epoch: 6, loss: 0.122, val_loss: 0.076\n",
      "epoch: 7, loss: 0.109, val_loss: 0.067\n",
      "epoch: 8, loss: 0.0967, val_loss: 0.060\n",
      "epoch: 9, loss: 0.0875, val_loss: 0.055\n",
      "epoch: 10, loss: 0.0803, val_loss: 0.052\n",
      "epoch: 11, loss: 0.0746, val_loss: 0.048\n",
      "epoch: 12, loss: 0.0691, val_loss: 0.044\n",
      "epoch: 13, loss: 0.0644, val_loss: 0.041\n",
      "epoch: 14, loss: 0.0604, val_loss: 0.039\n",
      "epoch: 15, loss: 0.0569, val_loss: 0.037\n",
      "epoch: 16, loss: 0.0537, val_loss: 0.035\n",
      "epoch: 17, loss: 0.0507, val_loss: 0.033\n",
      "epoch: 18, loss: 0.0481, val_loss: 0.031\n",
      "epoch: 19, loss: 0.0458, val_loss: 0.030\n",
      "epoch: 20, loss: 0.0437, val_loss: 0.029\n",
      "epoch: 21, loss: 0.0417, val_loss: 0.027\n",
      "epoch: 22, loss: 0.0399, val_loss: 0.026\n",
      "epoch: 23, loss: 0.0383, val_loss: 0.025\n",
      "epoch: 24, loss: 0.0368, val_loss: 0.024\n",
      "epoch: 25, loss: 0.0354, val_loss: 0.023\n",
      "epoch: 26, loss: 0.0341, val_loss: 0.022\n",
      "epoch: 27, loss: 0.0329, val_loss: 0.022\n",
      "epoch: 28, loss: 0.0318, val_loss: 0.021\n",
      "epoch: 29, loss: 0.0307, val_loss: 0.020\n",
      "epoch: 30, loss: 0.0298, val_loss: 0.020\n",
      "epoch: 31, loss: 0.0289, val_loss: 0.019\n",
      "epoch: 32, loss: 0.028, val_loss: 0.019\n",
      "epoch: 33, loss: 0.0272, val_loss: 0.018\n",
      "epoch: 34, loss: 0.0264, val_loss: 0.018\n",
      "epoch: 35, loss: 0.0257, val_loss: 0.017\n",
      "epoch: 36, loss: 0.025, val_loss: 0.017\n",
      "epoch: 37, loss: 0.0244, val_loss: 0.016\n",
      "epoch: 38, loss: 0.0238, val_loss: 0.016\n",
      "epoch: 39, loss: 0.0232, val_loss: 0.015\n",
      "epoch: 40, loss: 0.0227, val_loss: 0.015\n",
      "epoch: 41, loss: 0.0221, val_loss: 0.015\n",
      "epoch: 42, loss: 0.0216, val_loss: 0.014\n",
      "epoch: 43, loss: 0.0212, val_loss: 0.014\n",
      "epoch: 44, loss: 0.0207, val_loss: 0.014\n",
      "epoch: 45, loss: 0.0203, val_loss: 0.014\n",
      "epoch: 46, loss: 0.0199, val_loss: 0.013\n",
      "epoch: 47, loss: 0.0195, val_loss: 0.013\n",
      "epoch: 48, loss: 0.0191, val_loss: 0.013\n",
      "epoch: 49, loss: 0.0187, val_loss: 0.013\n",
      "epoch: 50, loss: 0.0184, val_loss: 0.012\n",
      "epoch: 51, loss: 0.018, val_loss: 0.012\n",
      "epoch: 52, loss: 0.0177, val_loss: 0.012\n",
      "epoch: 53, loss: 0.0174, val_loss: 0.012\n",
      "epoch: 54, loss: 0.0171, val_loss: 0.011\n",
      "epoch: 55, loss: 0.0168, val_loss: 0.011\n",
      "epoch: 56, loss: 0.0165, val_loss: 0.011\n",
      "epoch: 57, loss: 0.0162, val_loss: 0.011\n",
      "epoch: 58, loss: 0.016, val_loss: 0.011\n",
      "epoch: 59, loss: 0.0157, val_loss: 0.011\n",
      "epoch: 60, loss: 0.0155, val_loss: 0.010\n",
      "epoch: 61, loss: 0.0152, val_loss: 0.010\n",
      "epoch: 62, loss: 0.015, val_loss: 0.010\n",
      "epoch: 63, loss: 0.0148, val_loss: 0.010\n",
      "epoch: 64, loss: 0.0146, val_loss: 0.010\n",
      "epoch: 65, loss: 0.0144, val_loss: 0.010\n",
      "epoch: 66, loss: 0.0142, val_loss: 0.010\n",
      "epoch: 67, loss: 0.014, val_loss: 0.009\n",
      "epoch: 68, loss: 0.0138, val_loss: 0.009\n",
      "epoch: 69, loss: 0.0136, val_loss: 0.009\n",
      "epoch: 70, loss: 0.0134, val_loss: 0.009\n",
      "epoch: 71, loss: 0.0132, val_loss: 0.009\n",
      "epoch: 72, loss: 0.0131, val_loss: 0.009\n",
      "epoch: 73, loss: 0.0129, val_loss: 0.009\n",
      "epoch: 74, loss: 0.0127, val_loss: 0.009\n",
      "epoch: 75, loss: 0.0126, val_loss: 0.009\n",
      "epoch: 76, loss: 0.0124, val_loss: 0.008\n",
      "epoch: 77, loss: 0.0123, val_loss: 0.008\n",
      "epoch: 78, loss: 0.0121, val_loss: 0.008\n",
      "epoch: 79, loss: 0.012, val_loss: 0.008\n",
      "epoch: 80, loss: 0.0119, val_loss: 0.008\n",
      "epoch: 81, loss: 0.0117, val_loss: 0.008\n",
      "epoch: 82, loss: 0.0116, val_loss: 0.008\n",
      "epoch: 83, loss: 0.0115, val_loss: 0.008\n",
      "epoch: 84, loss: 0.0113, val_loss: 0.008\n",
      "epoch: 85, loss: 0.0112, val_loss: 0.008\n",
      "epoch: 86, loss: 0.0111, val_loss: 0.008\n",
      "epoch: 87, loss: 0.011, val_loss: 0.008\n",
      "epoch: 88, loss: 0.0109, val_loss: 0.007\n",
      "epoch: 89, loss: 0.0108, val_loss: 0.007\n",
      "epoch: 90, loss: 0.0107, val_loss: 0.007\n",
      "epoch: 91, loss: 0.0105, val_loss: 0.007\n",
      "epoch: 92, loss: 0.0104, val_loss: 0.007\n",
      "epoch: 93, loss: 0.0103, val_loss: 0.007\n",
      "epoch: 94, loss: 0.0102, val_loss: 0.007\n",
      "epoch: 95, loss: 0.0101, val_loss: 0.007\n",
      "epoch: 96, loss: 0.01, val_loss: 0.007\n",
      "epoch: 97, loss: 0.00995, val_loss: 0.007\n",
      "epoch: 98, loss: 0.00986, val_loss: 0.007\n",
      "epoch: 99, loss: 0.00977, val_loss: 0.007\n",
      "epoch: 100, loss: 0.00968, val_loss: 0.007\n",
      "epoch: 101, loss: 0.0096, val_loss: 0.007\n",
      "epoch: 102, loss: 0.00951, val_loss: 0.007\n",
      "epoch: 103, loss: 0.00943, val_loss: 0.006\n",
      "epoch: 104, loss: 0.00935, val_loss: 0.006\n",
      "epoch: 105, loss: 0.00927, val_loss: 0.006\n",
      "epoch: 106, loss: 0.00919, val_loss: 0.006\n",
      "epoch: 107, loss: 0.00911, val_loss: 0.006\n",
      "epoch: 108, loss: 0.00904, val_loss: 0.006\n",
      "epoch: 109, loss: 0.00896, val_loss: 0.006\n",
      "epoch: 110, loss: 0.00889, val_loss: 0.006\n",
      "epoch: 111, loss: 0.00882, val_loss: 0.006\n",
      "epoch: 112, loss: 0.00875, val_loss: 0.006\n",
      "epoch: 113, loss: 0.00868, val_loss: 0.006\n",
      "epoch: 114, loss: 0.00861, val_loss: 0.006\n",
      "epoch: 115, loss: 0.00855, val_loss: 0.006\n",
      "epoch: 116, loss: 0.00848, val_loss: 0.006\n",
      "epoch: 117, loss: 0.00841, val_loss: 0.006\n",
      "epoch: 118, loss: 0.00835, val_loss: 0.006\n",
      "epoch: 119, loss: 0.00829, val_loss: 0.006\n",
      "epoch: 120, loss: 0.00823, val_loss: 0.006\n",
      "epoch: 121, loss: 0.00817, val_loss: 0.006\n",
      "epoch: 122, loss: 0.00811, val_loss: 0.006\n",
      "epoch: 123, loss: 0.00805, val_loss: 0.006\n",
      "epoch: 124, loss: 0.00799, val_loss: 0.006\n",
      "epoch: 125, loss: 0.00794, val_loss: 0.006\n",
      "epoch: 126, loss: 0.00788, val_loss: 0.005\n",
      "epoch: 127, loss: 0.00783, val_loss: 0.005\n",
      "epoch: 128, loss: 0.00777, val_loss: 0.005\n",
      "epoch: 129, loss: 0.00772, val_loss: 0.005\n",
      "epoch: 130, loss: 0.00767, val_loss: 0.005\n",
      "epoch: 131, loss: 0.00761, val_loss: 0.005\n",
      "epoch: 132, loss: 0.00756, val_loss: 0.005\n",
      "epoch: 133, loss: 0.00751, val_loss: 0.005\n",
      "epoch: 134, loss: 0.00747, val_loss: 0.005\n",
      "epoch: 135, loss: 0.00742, val_loss: 0.005\n",
      "epoch: 136, loss: 0.00737, val_loss: 0.005\n",
      "epoch: 137, loss: 0.00732, val_loss: 0.005\n",
      "epoch: 138, loss: 0.00728, val_loss: 0.005\n",
      "epoch: 139, loss: 0.00723, val_loss: 0.005\n",
      "epoch: 140, loss: 0.00719, val_loss: 0.005\n",
      "epoch: 141, loss: 0.00714, val_loss: 0.005\n",
      "epoch: 142, loss: 0.0071, val_loss: 0.005\n",
      "epoch: 143, loss: 0.00706, val_loss: 0.005\n",
      "epoch: 144, loss: 0.00702, val_loss: 0.005\n",
      "epoch: 145, loss: 0.00698, val_loss: 0.005\n",
      "epoch: 146, loss: 0.00694, val_loss: 0.005\n",
      "epoch: 147, loss: 0.0069, val_loss: 0.005\n",
      "epoch: 148, loss: 0.00686, val_loss: 0.005\n",
      "epoch: 149, loss: 0.00682, val_loss: 0.005\n",
      "epoch: 150, loss: 0.00678, val_loss: 0.005\n",
      "epoch: 151, loss: 0.00674, val_loss: 0.005\n",
      "epoch: 152, loss: 0.0067, val_loss: 0.005\n",
      "epoch: 153, loss: 0.00667, val_loss: 0.005\n",
      "epoch: 154, loss: 0.00663, val_loss: 0.005\n",
      "epoch: 155, loss: 0.00659, val_loss: 0.005\n",
      "epoch: 156, loss: 0.00655, val_loss: 0.005\n",
      "epoch: 157, loss: 0.00652, val_loss: 0.005\n",
      "epoch: 158, loss: 0.00648, val_loss: 0.005\n",
      "epoch: 159, loss: 0.00645, val_loss: 0.005\n",
      "epoch: 160, loss: 0.00641, val_loss: 0.005\n",
      "epoch: 161, loss: 0.00638, val_loss: 0.005\n",
      "epoch: 162, loss: 0.00635, val_loss: 0.004\n",
      "epoch: 163, loss: 0.00631, val_loss: 0.004\n",
      "epoch: 164, loss: 0.00628, val_loss: 0.004\n",
      "epoch: 165, loss: 0.00625, val_loss: 0.004\n",
      "epoch: 166, loss: 0.00622, val_loss: 0.004\n",
      "epoch: 167, loss: 0.00618, val_loss: 0.004\n",
      "epoch: 168, loss: 0.00615, val_loss: 0.004\n",
      "epoch: 169, loss: 0.00612, val_loss: 0.004\n",
      "epoch: 170, loss: 0.00609, val_loss: 0.004\n",
      "epoch: 171, loss: 0.00606, val_loss: 0.004\n",
      "epoch: 172, loss: 0.00603, val_loss: 0.004\n",
      "epoch: 173, loss: 0.006, val_loss: 0.004\n",
      "epoch: 174, loss: 0.00597, val_loss: 0.004\n",
      "epoch: 175, loss: 0.00594, val_loss: 0.004\n",
      "epoch: 176, loss: 0.00591, val_loss: 0.004\n",
      "epoch: 177, loss: 0.00589, val_loss: 0.004\n",
      "epoch: 178, loss: 0.00586, val_loss: 0.004\n",
      "epoch: 179, loss: 0.00583, val_loss: 0.004\n",
      "epoch: 180, loss: 0.0058, val_loss: 0.004\n",
      "epoch: 181, loss: 0.00578, val_loss: 0.004\n",
      "epoch: 182, loss: 0.00575, val_loss: 0.004\n",
      "epoch: 183, loss: 0.00572, val_loss: 0.004\n",
      "epoch: 184, loss: 0.00569, val_loss: 0.004\n",
      "epoch: 185, loss: 0.00567, val_loss: 0.004\n",
      "epoch: 186, loss: 0.00564, val_loss: 0.004\n",
      "epoch: 187, loss: 0.00562, val_loss: 0.004\n",
      "epoch: 188, loss: 0.00559, val_loss: 0.004\n",
      "epoch: 189, loss: 0.00557, val_loss: 0.004\n",
      "epoch: 190, loss: 0.00554, val_loss: 0.004\n",
      "epoch: 191, loss: 0.00552, val_loss: 0.004\n",
      "epoch: 192, loss: 0.0055, val_loss: 0.004\n",
      "epoch: 193, loss: 0.00547, val_loss: 0.004\n",
      "epoch: 194, loss: 0.00545, val_loss: 0.004\n",
      "epoch: 195, loss: 0.00543, val_loss: 0.004\n",
      "epoch: 196, loss: 0.0054, val_loss: 0.004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 197, loss: 0.00538, val_loss: 0.004\n",
      "epoch: 198, loss: 0.00536, val_loss: 0.004\n",
      "epoch: 199, loss: 0.00533, val_loss: 0.004\n",
      "epoch: 200, loss: 0.00531, val_loss: 0.004\n",
      "epoch: 201, loss: 0.00529, val_loss: 0.004\n",
      "epoch: 202, loss: 0.00527, val_loss: 0.004\n",
      "epoch: 203, loss: 0.00525, val_loss: 0.004\n",
      "epoch: 204, loss: 0.00522, val_loss: 0.004\n",
      "epoch: 205, loss: 0.0052, val_loss: 0.004\n",
      "epoch: 206, loss: 0.00518, val_loss: 0.004\n",
      "epoch: 207, loss: 0.00516, val_loss: 0.004\n",
      "epoch: 208, loss: 0.00514, val_loss: 0.004\n",
      "epoch: 209, loss: 0.00512, val_loss: 0.004\n",
      "epoch: 210, loss: 0.0051, val_loss: 0.004\n",
      "epoch: 211, loss: 0.00508, val_loss: 0.004\n",
      "epoch: 212, loss: 0.00506, val_loss: 0.004\n",
      "epoch: 213, loss: 0.00504, val_loss: 0.004\n",
      "epoch: 214, loss: 0.00502, val_loss: 0.004\n",
      "epoch: 215, loss: 0.005, val_loss: 0.004\n",
      "epoch: 216, loss: 0.00498, val_loss: 0.004\n",
      "epoch: 217, loss: 0.00496, val_loss: 0.004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f3a9339760f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-f3a9339760f4>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, t)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c8d2dd7220f2>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_recurrent_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m     return super(SimpleRNN, self).call(\n\u001b[0;32m-> 1393\u001b[0;31m         inputs, mask=mask, training=training, initial_state=initial_state)\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         zero_output_for_mask=self.zero_output_for_mask)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4154\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4155\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4156\u001b[0;31m           **while_loop_kwargs)\n\u001b[0m\u001b[1;32m   4157\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4138\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4139\u001b[0m         output, new_states = step_function(current_input,\n\u001b[0;32m-> 4140\u001b[0;31m                                            tuple(states) + tuple(constants))\n\u001b[0m\u001b[1;32m   4141\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4142\u001b[0m         \u001b[0mflat_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    730\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_rnn_cell\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m           \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m   1221\u001b[0m       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m       \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrec_dp_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(x, bias, data_format)\u001b[0m\n\u001b[1;32m   5544\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5545\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5546\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5547\u001b[0m   \u001b[0;31m# pylint: enable=g-no-augmented-assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5548\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2716\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2718\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BiasAdd\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         data_format)\n\u001b[0m\u001b[1;32m    743\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,amsgrad=True)\n",
    "\n",
    "train_loss = metrics.Mean()\n",
    "val_loss = metrics.Mean()\n",
    "\n",
    "def compute_loss(t,y):\n",
    "    return criterion(t,y)\n",
    "def train_step(x,t):\n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = model(x)\n",
    "        loss = compute_loss(t,preds)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    return loss\n",
    "\n",
    "def val_step(x,t):\n",
    "    preds = model(x)\n",
    "    loss = compute_loss(t, preds)\n",
    "    val_loss(loss)\n",
    "    \n",
    "epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "n_batches_train = x_train.shape[0]//batch_size+1\n",
    "n_batches_val = x_val.shape[0]//batch_size +1\n",
    "hist = {'loss':[],'val_loss':[]}\n",
    "es = EarlyStopping(patience=10, verbose=1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x_, t_ = shuffle(x_train, t_train)\n",
    "    \n",
    "    for batch in range(n_batches_train):\n",
    "        start = batch*batch_size\n",
    "        end = start +batch_size\n",
    "        train_step(x_[start:end],t_[start:end])\n",
    "        \n",
    "    for batch in range(n_batches_val):\n",
    "        start = batch*batch_size\n",
    "        end = start +batch_size\n",
    "        val_step(x_[start:end],t_[start:end])\n",
    "        \n",
    "    hist['loss'].append(train_loss.result())\n",
    "    hist['val_loss'].append(val_loss.result())\n",
    "    \n",
    "    print('epoch: {}, loss: {:.3}, val_loss: {:.3f}'.format(\n",
    "            epoch+1,\n",
    "            train_loss.result(),\n",
    "            val_loss.result()\n",
    "        ))\n",
    "    \n",
    "    if es(val_loss.result()):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from callbacks import EarlyStopping\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.RNN(1, hidden_dim,\n",
    "                         nonlinearity='tanh',\n",
    "                         batch_first=True)\n",
    "        self.l2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        nn.init.xavier_normal_(self.l1.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.l1.weight_hh_l0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.l1(x)\n",
    "        y = self.l2(h[:, -1])\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(50).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNN' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5e01b0b80e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.009\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mamsgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RNN' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(reduction = 'mean')\n",
    "optimizer = optimizers.Adam(model.parameters(), lr=0.01,betas=(0.9,0.009),amsgrad=True)\n",
    "\n",
    "\n",
    "def compute_loss(t,y):\n",
    "    return criterion(y,t)\n",
    "\n",
    "def train_step(x,t):\n",
    "    x = torch.Tensor(x).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tabata/opt/anaconda3/envs/deeplearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/tabata/opt/anaconda3/envs/deeplearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/Users/tabata/opt/anaconda3/envs/deeplearning/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, activation = 'tanh',\n",
    "              recurrent_activation='sigmoid',#各ゲートにおける活性化関数\n",
    "              kernel_initializer='glorot_normal',\n",
    "              recurrent_initializer='orthogonal'),\n",
    "         )\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "from callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Model):\n",
    "    def __init__(self,hidden_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = LSTM(hidden_dim,activation = 'tanh',\n",
    "                      recurrent_activation= 'sigmoid',\n",
    "                      kernel_initializer='glorot_normal',\n",
    "                      recurrent_initializer='orthogonal')\n",
    "        self.l2 = Dense(1, activationn = 'linear')\n",
    "        \n",
    "    def call(self, x):\n",
    "        h = self.l1(x)\n",
    "        y = self.l2(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optimizers\n",
    "from callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.LSTM(1, hidden_dim, batch_first = True)\n",
    "        self.l2 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        nn.init.xavier_normal_(self.l1.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.l1.weight_hh_l0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h,_ = self.l1(x)\n",
    "        y = self.l2(h[:,-1])\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
